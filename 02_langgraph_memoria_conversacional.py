from langgraph.graph import StateGraph, END

# Definimos el "estado" compartido como un diccionario
# Aqu칤 podr칤as usar TypedDict para mayor robustez, pero para este ejemplo usamos dict
class Estado(dict):
    pass  # El estado guardar치 el historial de la conversaci칩n y el 칰ltimo input

# --- Nodos ---
# Nodo encargado de recibir el input del usuario y actualizar el historial
def nodo_input(state: Estado):
    user_input = input("游녻 Usuario: ")  # Solicita entrada al usuario
    # Si no existe el historial, lo crea como lista vac칤a
    state.setdefault("historial", [])
    # A침ade el mensaje del usuario al historial
    state["historial"].append({"role": "user", "content": user_input})
    # Guarda el 칰ltimo input para poder controlar la salida del bucle
    state["ultimo_input"] = user_input
    return state  # Devuelve el estado actualizado

# Nodo que simula la respuesta de un modelo LLM usando el historial
def nodo_llm(state: Estado):
    # Obtiene el historial de la conversaci칩n
    historial = state.get("historial", [])
    # Toma la 칰ltima pregunta del usuario (si existe)
    ult_pregunta = historial[-1]["content"] if historial else ""
    # Genera una respuesta simulada usando la 칰ltima pregunta y el tama침o del historial
    respuesta = f"游뱄 LLM responde a: '{ult_pregunta}' teniendo en cuenta el historial ({len(historial)} turnos)."
    # A침ade la respuesta del asistente al historial
    historial.append({"role": "assistant", "content": respuesta})
    state["historial"] = historial  # Actualiza el historial en el estado
    print(respuesta)  # Muestra la respuesta por pantalla
    return state  # Devuelve el estado actualizado

# --- Grafo ---
workflow = StateGraph(Estado)

workflow.add_node("input", nodo_input)
workflow.add_node("llm", nodo_llm)

workflow.set_entry_point("input")
workflow.add_edge("input", "llm")
workflow.add_edge("llm", "input")  # bucle para conversaci칩n
workflow.add_edge("input", END)    # permite salir si se corta

# Compilamos el grafo (aunque en este ejemplo ejecutamos los nodos manualmente)
grafo = workflow.compile()

# --- Ejecuci칩n principal del chat ---
print("=== Chat con memoria (LangGraph) ===")
# Inicializamos el estado con un historial vac칤o
estado = Estado(historial=[])
print("(Escribe 'exit' para salir)")
while True:
    # 1. Pedimos input al usuario y lo guardamos en el historial
    estado = nodo_input(estado)
    # 2. Si el usuario escribe 'exit', terminamos la conversaci칩n
    if estado.get("ultimo_input", "").strip().lower() == "exit":
        print("游녦 Conversaci칩n finalizada.")
        break
    # 3. El "modelo" responde usando el historial actualizado
    estado = nodo_llm(estado)

print("游늷 Historial final:", estado["historial"])
